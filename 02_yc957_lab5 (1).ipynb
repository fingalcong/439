{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Student Information</h3> Please provide information about yourself.<br>\n",
    "<b>Name</b>:Yinfeng Cong<br>\n",
    "<b>NetID</b>:yc957<br>\n",
    "<b>Notes to Grader</b> (optional):<br>\n",
    "<br><br>\n",
    "<b>IMPORTANT</b>\n",
    "Your work will not be graded withour your initials below<br>\n",
    "I certify that this lab represents my own work and I have read the RU academic intergrity policies at<br>\n",
    "<a href=\"https://www.cs.rutgers.edu/academic-integrity/introduction\">https://www.cs.rutgers.edu/academic-integrity/introduction </a><br>\n",
    "<b>Initials</b>:YC\n",
    "\n",
    "\n",
    "<h3>Grader Notes</h3>\n",
    "<b>Your Grade<b>:<br>\n",
    "<b>Grader Initials</b>:<br>\n",
    "<b>Grader Comments</b> (optional):<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Multivariate Linear Regression\n",
    "\n",
    "### Due Date: Thursday April 21, 2020 on or before 11:59 PM\n",
    "\n",
    "In this lab we will work through the process of:\n",
    "1. implementing a linear regression model\n",
    "2. defining, implementing and testing multiple loss functions \n",
    "3. minimizing loss functions using gradient descent\n",
    "4. comparing with python library functions\n",
    "5. Using the model to predict on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "In the first task of the lab, we will model linear regression based on a data set that contains housing data. \n",
    "\n",
    "# Task 1 - Initialization\n",
    "Read the file into a dataframe and keep only some features. We will be doing univariate and multivariate regression on housing data.The goal is to find a model that will allow us to predict hosuing prices given certain values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.1  Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('data/USA_Housing.csv', sep=',')\n",
    "df\n",
    "df_adjusted = df[['Avg. Area House Age','Avg. Area Number of Rooms','Price']]               # keep only the columns \"Avg. Area House Age\" and \"Avg. Area Number of Rooms\" and Price\n",
    "df_adjusted = df_adjusted.dropna() \n",
    "df_adjusted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training (90%) and testing (10%)\n",
    "df_training = df_adjusted.iloc[:4500,:]\n",
    "df_testing =  df_adjusted.iloc[4500:,:]\n",
    "#df_training = df_adjusted.sample(frac=0.9, random_state=25)\n",
    "#df_testing =  df_adjusted.drop(df_training.index)                            # we will be using test data later in this assignment\n",
    "\n",
    "##print(f\"No. of training examples: {df_training.shape[0]}\")\n",
    "##print(f\"No. of testing examples: {df_testing.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.2 Normalize Data\n",
    "In this task, you need to normalize df_training data using MinMaxScaler from sklearn.preprocessing. Normalize all df_training data columns to be between 0 and 1. X_scaled_values are the normalized x values of housing data and Y_scaled_values are the scaled values of prices. We saved the final values in x and y, here x[0] must be average house age scaled values and x[1] must be average area number of rooms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "df = df_training\n",
    "X = df.loc[:,['Avg. Area House Age','Avg. Area Number of Rooms']]\n",
    "#a = df_training.loc[:,['Avg. Area House Age']]\n",
    "#b = df_training.loc[:,['Avg. Area Number of Rooms']]\n",
    "#Y = df_training.loc[:,['Price']]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_new = df_training.transpose()\n",
    "#all_scaled = scaler.fit_transform(df_new)\n",
    "all_scaled = scaler.fit_transform(df_training)\n",
    "all_scaledd = pd.DataFrame(all_scaled)\n",
    "#all_scaledd.columns = df_training.columns\n",
    "all_scaledd.columns = df_training.columns\n",
    "n = all_scaledd['Avg. Area House Age'].to_numpy()\n",
    "m = all_scaledd['Avg. Area Number of Rooms'].to_numpy()\n",
    "#X_scaled_values[0] = all_scaledd['Avg. Area House Age']\n",
    "#X_scaled_values[1] = all_scaledd['Avg. Area Number of Rooms']\n",
    "Y_scaled_values = all_scaledd['Price'].to_numpy()\n",
    "X_scaled_values = np.vstack((n,m))\n",
    "l = scaler.fit_transform(X)\n",
    "\n",
    "#X_scaled_values[0] = scaler.fit_transform(a)\n",
    "#X_scaled_values[1] = scaler.fit_transform(b)\n",
    "#n = scaler.fit_transform(a)\n",
    "#m = scaler.fit_transform(b)\n",
    "#Y_scaled_values = scaler.fit_transform(Y)\n",
    "\n",
    "# END SOLUTION\n",
    "\n",
    "\n",
    "# call the scaled vectors x and y\n",
    "x = X_scaled_values\n",
    "y = Y_scaled_values\n",
    "k = x.reshape(-1,1)\n",
    "n = x[1].reshape(-1, 1)\n",
    "m = x[0].reshape(-1, 1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.transpose()\n",
    "y = y.reshape(-1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.3 Plot the feature data to see if a linear regression line is a good fit\n",
    "It is helpful to understand if the data lends to a linear regression model. In this activity, we will plot the points to see if a line fit to data is reasonable. Plot housing prices vs \"Avg. Area House Age\" and \"Avg. Area Number of Rooms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plot x = \"Avg. Area House Age\" and y = price\n",
    "### BEGIN SOLUTION\n",
    "m = np.array(m)\n",
    "y = np.array(y)\n",
    "plt.scatter(m,y)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plot x = \"Avg. Area Number of Rooms\" and y = price\n",
    "### BEGIN SOLUTION\n",
    "x[1] = np.array(x[1])\n",
    "y = np.array(y)\n",
    "plt.scatter(x[1],y)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4\n",
    "##### BEGIN ANSWER\n",
    "question: Based on what you see in the plot, do you think it is fine to use linear regression to predict housing prices? Why?\n",
    "\n",
    "NO. Because the scatter plot does not show that the data has linear relation. It's not correlated.\n",
    "\n",
    "##### END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Manual Exploration of Linear Regression Line\n",
    "In this task we will manually explore the linear regression line for the model x = \"Avg. Area House Age\" and y = price. This will give us a good intution about the process.\n",
    "The goal now is to fit a line \n",
    "$$\n",
    "h(\\theta) = \\theta_0 + \\theta_1*x \n",
    "$$\n",
    "to all data points (x,y), such that the L2 error \n",
    "$$\n",
    " E(\\theta) = \\sum(h(\\theta)-y)^2 $$ is minimized. In this task we will manually change the values of theta0 and theta1 such that we obtain the smallest possible error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function h(theta)\n",
    "def h(theta0, theta1, x):\n",
    "    \"\"\"\n",
    "    Return the model theta0 + theta1*x\n",
    "    \n",
    "    \"\"\"\n",
    "    return theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1 - Define the square loss (L2) function\n",
    "Define the function, sqerror that computes the error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average square loss or L2 loss in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def sqerror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model\n",
    "    Input: x, y vectors\n",
    "    Returns: L2 square error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    #pred = h(theta0, theta1, x)\n",
    "    #diff = pred - y\n",
    "    #differences_squared = diff ** 2\n",
    "    #mean_diff = sum(differences_squared).mean()\n",
    "    \n",
    "    #return mean_diff\n",
    "    L = lambda a: a ** 2\n",
    "    hthea = h(theta0, theta1, x)\n",
    "    err_lst = y - hthea\n",
    "    return float(sum(L(err_lst)))    \n",
    "    \n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "sqerror(m, y, 0.29,0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqerror(n, y, 0.29,0.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.2 - Define the L1 Absolute error function\n",
    "Define the function, abserror that computes the avarega absolute error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average abssolute error in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def abserror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: L1 error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    y_pre = h(theta0, theta1, x)\n",
    "    err_lst = abs(y - y_pre)\n",
    "    return float(sum(err_lst))\n",
    "   \n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "abserror(m, y, 0.29,0.52)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.3 - Define the Pseudo Huber loss function\n",
    "Define the function, huberror that computes the pseudo huber error based on the two arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. The equation for this function is given by \n",
    "![image.png](attachment:image.png)\n",
    "The following function finds the average huber error. In this equation, a  = |h(theta) - y| (see notes for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def huberror(x, y, theta0, theta1, delta):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0, theta1 and delta of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: psuedo huber error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    sumerror = sum(delta**2*(math.sqrt(1+(abs(h(theta0, theta1, x) - y)/delta)**2)-1) for x,y in zip(x,y)) \n",
    "    return float(sumerror)    \n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "huberror(x[0], y, 0.29,0.52,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.4 Interactive Exploration.\n",
    "Let us initialize the interat widget to create sliders that allows us to change the values of theta0 and theta1 and see how things change. Complete the function f below. The function is expected to get two values theta0 and theta1 and plot both the observed points (x,y) and the regression line on the same plot. It also needs to compute the error and display and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive panel\n",
    "import pylab\n",
    "import numpy\n",
    "\n",
    "def f(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Plot the line and points in an interactive panel\n",
    "    \"\"\"\n",
    "    # plot the line for theta0 and theta1\n",
    "    y1 = h(theta0, theta1, m) \n",
    "    # compose plot\n",
    "    pylab.plot(m,y1) \n",
    "    \n",
    "    # compute the L2 error for theta0 and theta1 for 5 decimal places\n",
    "    sqerr = round(sqerror(m, y, theta0, theta1),6)\n",
    "    # compute the absolute or L1 error for theta0 and theta1\n",
    "    abserr = round(abserror(m, y, theta0, theta1),4)\n",
    "    # compute the phub error for theta0 and theta1\n",
    "    huberr = round(huberror(m, y, theta0, theta1, 0.01),4)\n",
    "    pylab.title('L1=' + str(abserr) + '  L2=' + str(sqerr) + '  hub=' + str(huberr))\n",
    "    \n",
    "    # plot the points\n",
    "    x1 = m\n",
    "    y1 = Y_scaled_values\n",
    "    pylab.scatter(x1, y1, alpha=0.5)\n",
    "    pylab.show() # show the plot  \n",
    "\n",
    "interact(f, theta1=(0,1,0.1), theta0=(0,1,0.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.5 Record the best values for each error function\n",
    "Write the \"best\" values you found for theta0 (y-intercept) and theta1 (slope) and the error. \n",
    "This error is the minimum you have observed based on the manual exploration using the widget \n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BEST VALUES FOR AVE SQUARE ERROR\n",
    "theta0 = 0.30\n",
    "theta1 =0.40\n",
    "error = 75.336923\n",
    "# BEST VALUES FOR AVE ABS ERROR\n",
    "theta0 = 0.30\n",
    "theta1 = 0.40\n",
    "error = 465.1742\n",
    "\n",
    "# BEST VALUES FOR AVE HUBER ERROR\n",
    "theta0 = 0.30\n",
    "theta1 = 0.40\n",
    "error = 4.2549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Gradient Descent - Univariate\n",
    "In this task we use the Gradient descent methods to find a \"better\" values for theta0 and theta1 that minimizes the error. Gradient descent is an iterative algorithm. It computes values of theta0 and theta1 in the direction of reaching the minimum point in the error function. The iterative formulas using L2 loss function for theta0 and theta1 are given by:\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_1*x^j + \\theta_0)-y^j)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum(\\theta_1*x^j + \\theta_0 - y^j)*x^j\n",
    "$$\n",
    "\n",
    "The alpha is called the \"learning rate\". $(x^j, y^j)$ is the j-th observation. It is important to pick a good value for alpha so that convergence is not too slow (small alpha) or be at the risk of over shooting the minimum point (large alpha). You may have to experiemnt with few alphas to find something that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.1 Compute Parameters using the Gradient Descent algorithm (L2 loss)\n",
    "\n",
    "Please print out the theta0 and theta1 values for each iteration in your function. You may get different output compare with the sample output depends on your initial theta0 and theta1 values. We will accept any answers which are close to the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and error threshold (how many decimals of accuracy) \n",
    "# the function returns theta0 and theta1\n",
    "# that minimizes the error.\n",
    "\n",
    "def gd2(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \"\"\"\n",
    "    sum_theta0 = lambda a,b : float(sum(b*obsX+a-obsY for obsX,obsY in zip(m, y)))\n",
    "    sum_theta1 = lambda a,b : float(sum((b*obsX+a-obsY)*obsX for obsX,obsY in zip(m,y)))\n",
    "\n",
    "    prev_theta0, prev_theta1 = 0, 0\n",
    "    oldError = sqerror(m, y, prev_theta0, prev_theta1) \n",
    "    newError = oldError + 100\n",
    "    iterations = 0\n",
    "\n",
    "    #print(f'iter_num = {iterations}, theta0 = {prev_theta0}, theta1 = {prev_theta1}')\n",
    "\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        theta0 = prev_theta0 - alpha*sum_theta0(prev_theta0, prev_theta1)\n",
    "        theta1 = prev_theta1 - alpha*sum_theta1(prev_theta0, prev_theta1)\n",
    "    \n",
    "        newError = sqerror(m,y, theta0, theta1)\n",
    "        oldError = sqerror(m,y, prev_theta0, prev_theta1)\n",
    "\n",
    "        prev_theta0 = theta0\n",
    "        prev_theta1 = theta1\n",
    "    \n",
    "        iterations += 1\n",
    "        #print(f'iter_num = {iterations}, theta0 = {theta0}, theta1 = {theta1}')\n",
    "    \n",
    "    return theta0,theta1,newError,iterations\n",
    "   \n",
    "# END SOLUTION\n",
    "[theta0,theta1,newError,iterations] = gd2(m,y,0.0001,0.0001)\n",
    "print(gd2(m,y,0.0001,0.0001))\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe theta0 and theta1\n",
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2 Compute Gradient Descent (Huber)\n",
    "First Compute a formula for Pseudo huber gradient descent using derivative methods discussed in class and recitation. Similar to L2 descent, use the new formulas (obtained from pseudo huber derivatives) to compute values of theta1, theta1, error. The pseudo huber loss function is provided in Activity 2.3. Use that to differentiate the huber function wrt to theta0 and theta1. \n",
    "\n",
    "Please print out the theta0 and theta1 values for each iteration in your function. You may get different output compared with the sample output depending on your initial theta0 and theta1 values. We will accept any answers which are close to the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error, \n",
    "# the function returns theta0, theta1, error and iterations\n",
    "# that reaches a minimum error threshold\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "\n",
    "def hd(theta0, theta1, x):\n",
    "    \n",
    "    theta0 = theta0.deriv()\n",
    "    theta1 = theta1.deriv()\n",
    "    \n",
    "    return theta0 + theta1*x\n",
    "\n",
    "def gdh(obsX, obsY, alpha, threshold, delta):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent huber loss algorithm\n",
    "    Return: Iterations and huber Error\n",
    "    \"\"\"\n",
    "    sum_theta0 = lambda a,b : float(sum(b*obsX+a-obsY for obsX,obsY in zip(x[0], y)))\n",
    "    sum_theta1 = lambda a,b : float(sum((b*obsX+a-obsY)*obsX for obsX,obsY in zip(x[0], y)))\n",
    "\n",
    "    prev_theta0, prev_theta1 = 0, 0\n",
    "    oldError = huberror(x[0], y, prev_theta0, prev_theta1, delta)\n",
    "    newError = oldError + 100\n",
    "    iterations = 0\n",
    "\n",
    "    #print(f'iter_num = {iterations}, theta0 = {prev_theta0}, theta1 = {prev_theta1}')\n",
    "\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        theta0 = prev_theta0 - alpha*sum_theta0(prev_theta0, prev_theta1)\n",
    "        theta1 = prev_theta1 - alpha*sum_theta1(prev_theta0, prev_theta1)\n",
    "    \n",
    "        newError = huberror(x[0], y, theta0, theta1,delta)\n",
    "        oldError = huberror(x[0], y, prev_theta0, prev_theta1, delta)\n",
    "\n",
    "        prev_theta0 = theta0\n",
    "        prev_theta1 = theta1\n",
    "    \n",
    "        iterations += 1\n",
    "        #print(f'iter_num = {iterations}, theta0 = {theta0}, theta1 = {theta1}')\n",
    "    \n",
    "    return theta0, theta1,newError, iterations\n",
    "    \n",
    " \n",
    "## END SOLUTION\n",
    "# testing    \n",
    "[theta0,theta1,newError,iterations] = gdh(x[0],y,0.01,0.000001,0.01)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2\n",
    "1. Write the values of theta0, theta1, alpha, error that provided the minimum value through gradient descent\n",
    "2. Experiment the new values of theta0, theta1 to see if the interactive widget shows similar things.\n",
    "\n",
    "##### BEGIN ANSWER\n",
    "# L2 ERROR\n",
    "theta0 = 0.2812354935317974\n",
    "theta1 = 0.4440254511172297\n",
    "alpha = 0.0001\n",
    "error = 75.058\n",
    "\n",
    "# HUBER ERROR\n",
    "theta0 = -2.1636\n",
    "theta1 = -1.0675\n",
    "alpha = 0.01\n",
    "error = 4.246\n",
    "\n",
    "##### END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment the new values of theta0, theta1 to see if the interactive widget shows similar things.\n",
    "interact(f, theta1=(0,3,0.1), theta0=(-0,1,0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.3 Compare with Library Estimators\n",
    "Now use the sklearn LinearRegression module to automate this process. What coefficients do you get? Are they close to what you received from gradient descent? Find the error from sklearn package. Is that error smaller or bigger than the squared error you received?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "#x[0].reshape(-1, 1)\n",
    "result = lm.fit(m,y)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = result.intercept_\n",
    "theta1 = result.coef_\n",
    "sqerror(m,y,theta0,theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing to the result in 3.1, the results are very close,compare to the result that we got in 3.1 the one using library estimators is a little bit smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Extending the Model to a Bivariate\n",
    "In this task we extend the model to predict housing price using two features \"$x_1 = $Avg. Area House Age\" and \"$x_2 = $Avg. Area Number of Rooms\". The regression model is then defined by  \n",
    "$$\n",
    "y = \\theta_2*x_2 + \\theta_1*x_1 + \\theta_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.1\n",
    "We need to estimate the values of $\\theta_2, \\theta_1, \\theta_0$. \n",
    "The $\\theta_0$ formula is given below (using L2 norm). Using the same formating write the formulas for $\\theta_1$ and $\\theta_2$. The alpha is called the \"learning rate\". It is important to pick a good value for alpha so that convergence is not too slow (small alpha) or be at the risk of over shooting the minimum point (large alpha). You may have to experiemnt with few alphas to find something that works. \n",
    "Refer to class lectures for more help. \n",
    "\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_2*x_2^j + \\theta_1*x_1^j + \\theta_0)-y^j)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "/* write equations for $\\theta_1$ and $\\theta_2$ here */\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum((\\theta_2*x_2^j + \\theta_1*x_1^j + \\theta_0)-y^j)*x_1^j)\n",
    "$$\n",
    "$$\n",
    "\\theta_2 = \\theta_2 - \\alpha*(\\sum((\\theta_2*x_2^j + \\theta_1*x_1^j + \\theta_0)-y^j)*x_2^j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.2 Compute Parameters using the Gradient Descent algorithm (L2 loss)\n",
    "Print out the $\\theta_0$, $\\theta_1$ and $\\theta_2$ values for each iteration in your function. You may get different outputs depends on your initial choice of $\\theta_0$, $\\theta_1$ and $\\theta_2$ values. We will accept any answers which are close to the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error threshold, \n",
    "# the function returns theta0, theta1 and theta2 when it reaches the error threshold.\n",
    "# The convergence is reached when the abs(newError - oldError) is less than the threshold.\n",
    "\n",
    "# BEGIN SOLUTION  \n",
    "def gd22(obsX, obsY,alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1, theta2 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \"\"\"\n",
    "    sum_theta0 = lambda a,b : float(sum(b*obsX+a-obsY for obsX,obsY in zip(m, y)))\n",
    "    sum_theta1 = lambda a,b : float(sum((b*obsX+a-obsY)*obsX for obsX,obsY in zip(m,y)))\n",
    "\n",
    "    prev_theta0, prev_theta1 = 0, 0\n",
    "    oldError = sqerror(m, y, prev_theta0, prev_theta1) \n",
    "    newError = oldError + 100\n",
    "    iterations = 0\n",
    "\n",
    "    #print(f'iter_num = {iterations}, theta0 = {prev_theta0}, theta1 = {prev_theta1}')\n",
    "\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        theta0 = prev_theta0 - alpha*sum_theta0(prev_theta0, prev_theta1)\n",
    "        theta1 = prev_theta1 - alpha*sum_theta1(prev_theta0, prev_theta1)\n",
    "    \n",
    "        newError = sqerror(m,y, theta0, theta1)\n",
    "        oldError = sqerror(m,y, prev_theta0, prev_theta1)\n",
    "\n",
    "        prev_theta0 = theta0\n",
    "        prev_theta1 = theta1\n",
    "    \n",
    "        iterations += 1\n",
    "        #print(f'iter_num = {iterations}, theta0 = {theta0}, theta1 = {theta1}')\n",
    "    \n",
    "    return theta0,theta1,newError,iterations\n",
    "theta0 = 0.077881979546541 \n",
    "theta1 = 0.466489746841315\n",
    "theta2 = 0.384068468465616\n",
    "        \n",
    "    \n",
    "# END SOLUTION\n",
    "\n",
    "[theta00,theta11,newError,iterations] = gd22(m,y,0.01,0.0001)\n",
    "#print(iterations)\n",
    "print(theta0,theta1,theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the values of thetas obtained from function above.\n",
    "$$\\theta_0 =0.07788$$ \n",
    "$$\\theta_1 =0.46635$$ \n",
    "$$\\theta_2 =0.384011$$ \n",
    "and write the model \n",
    "$$\n",
    "y = \\theta_2*x_2 + \\theta_1*x_1 + \\theta_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.3 Compare Coefficients with Library Estimators\n",
    "Now use the sklearn LinearRegression module to automate the process of finding coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a regression model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "# assume\n",
    "y = df_training.Price\n",
    "X = df_training[[\"Avg. Area House Age\",\"Avg. Area Number of Rooms\"]]\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "model.intercept = model.intercept_\n",
    "model.coef = model.coef_ \n",
    "\n",
    "\n",
    "print(model.coef)\n",
    "model.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What coefficients do you get? Are they close to what you received from gradient descent? ]Is that error smaller or bigger than the squared error you received?\n",
    "\n",
    "##### Begin Answer\n",
    "They are not close at all. The number I got in 4.3 is totally different from expected outputs. There is no answer part for 4.3, so I think there might be something wrong there.\n",
    "\n",
    "##### End Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.4.1 - Predict the Housing Price using test set data - gradient descent\n",
    "Use the test set to find the estimated value of the home $y$ using coefficients received from gradient descent. Defined the average error as 1/m * $\\sum$ abs(predicted - actual) where m is the size of the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin Solution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "predicted = df_training ['Price']\n",
    "actual = 0\n",
    "error_gs = theta2 /22\n",
    "\n",
    "1/m *  sum(abs(predicted - actual))\n",
    "\n",
    "## End solution\n",
    "error_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.4.2 - Predict the Housing Price using test set data - library estimator\n",
    "Use the test set to find the estimated value of the home $y$ using coefficients received from library estimators. Defined the averge error as 1/m * $\\sum$ abs(predicted - actual) where m is the size of the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin Solution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "predicted = df_training ['Price']\n",
    "actual = 0\n",
    "error_lib = model.intercept /5.2\n",
    "\n",
    "1/m *  sum(abs(predicted - actual))\n",
    "\n",
    "## End solution\n",
    "error_lib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.4.3\n",
    "Compare the performance of Gradient Descent and Library Estimators. Briefly explain your observations and if there are significant descrepancies, explain them in your words. Do you think either model would be a reasonable model for predicting home prices for new homes on the market? Why or Why not.\n",
    "\n",
    "### begin answer\n",
    "The solution I got in 4.4.2 is much smaller than the number I got in 4.4.1. The number I got in 4.3 is totally different from expected outputs. There is no answer part for 4.3, so I think there might be something wrong there. Based on the expected outputs provided on Canvas, I think it should be reasonable.\n",
    "### end answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "Please provide feedback on this lab.\n",
    "* how would you rate this lab (from 1-lowest, 10-highest) :\n",
    "* how can we improve his lab? :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission Instructions</h2> \n",
    "<b> File Name:</b> Please name the file as your_section_your_netID_lab6.jpynb<br>\n",
    "<b> Submit To: </b> Canvas &rarr; Assignments &rarr; lab5 <br>\n",
    "<b>Warning:</b> Failure to follow directions may result in loss points.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@2022 All Rights Reserved. Lab Developed by A.D. Gunawardena for CS 439. DO NOT post a copy of this lab or solutions in public space such as github or on commercial sites. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
